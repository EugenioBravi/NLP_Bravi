{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "from gliner import GLiNER\n",
    "from deep_translator import MyMemoryTranslator\n",
    "import ast"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modelo Analisis de sentimientos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_sentimientos = pd.read_csv('./databases/sentimientos.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>clase</th>\n",
       "      <th>frase</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>me siento un poco nostalgico hoy.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>todo parece tan gris, como si algo faltara.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>no tengo ganas de hacer nada, estoy algo decaido.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>echo de menos esos tiempos felices que ya no v...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>me invade una sensacion de tristeza, sin motiv...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   clase                                              frase\n",
       "0      0                  me siento un poco nostalgico hoy.\n",
       "1      0        todo parece tan gris, como si algo faltara.\n",
       "2      0  no tengo ganas de hacer nada, estoy algo decaido.\n",
       "3      0  echo de menos esos tiempos felices que ya no v...\n",
       "4      0  me invade una sensacion de tristeza, sin motiv..."
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_sentimientos.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precisión Regresión Logística: 0.9393939393939394\n",
      "Reporte de clasificación Regresión Logística:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.91      0.91      0.91        11\n",
      "           1       0.91      0.91      0.91        11\n",
      "           2       1.00      1.00      1.00        11\n",
      "\n",
      "    accuracy                           0.94        33\n",
      "   macro avg       0.94      0.94      0.94        33\n",
      "weighted avg       0.94      0.94      0.94        33\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model = SentenceTransformer('sentence-transformers/distiluse-base-multilingual-cased-v1') \n",
    "\n",
    "labels = [(0, \"Melancolico\"), (1, \"Ni fu Ni fa\"), (2, \"Alegre\")]\n",
    "\n",
    "# División del dataset\n",
    "X_train, X_test, y_train, y_test = train_test_split(dataset_sentimientos['frase'].to_list(), dataset_sentimientos['clase'].to_list(), test_size=0.2, shuffle=True,random_state=42)\n",
    "\n",
    "# Obtenemos los embeddings de BERT para los conjuntos de entrenamiento y prueba\n",
    "X_train_vectorized = model.encode(X_train)\n",
    "X_test_vectorized = model.encode(X_test)\n",
    "# Creación y entrenamiento del modelo de Regresión Logística Multinomial\n",
    "modelo_sentimientos = LogisticRegression(max_iter=100, solver='lbfgs')\n",
    "modelo_sentimientos.fit(X_train_vectorized, y_train)\n",
    "\n",
    "# Evaluación del modelo de Regresión Logística\n",
    "y_pred_sentimientos = modelo_sentimientos.predict(X_test_vectorized)\n",
    "acc_sentimientos = accuracy_score(y_test, y_pred_sentimientos)\n",
    "report_sentimientos = classification_report(y_test, y_pred_sentimientos, zero_division=1)\n",
    "\n",
    "print(\"Precisión Regresión Logística:\", acc_sentimientos)\n",
    "print(\"Reporte de clasificación Regresión Logística:\\n\", report_sentimientos)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Diferencia encontrada en el índice 8:\n",
      "X_test: me siento como si el dia hubiera pasado desapercibido, sin nada memorable.\n",
      "y_test: 1\n",
      "y_pred_sentimientos: 0\n",
      "\n",
      "Diferencia encontrada en el índice 12:\n",
      "X_test: no encuentro consuelo en nada, todo parece sombrio.\n",
      "y_test: 0\n",
      "y_pred_sentimientos: 1\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(y_test)):\n",
    "    if y_test[i] != y_pred_sentimientos[i]:\n",
    "        print(f\"Diferencia encontrada en el índice {i}:\")\n",
    "        print(f\"X_test: {X_test[i]}\")\n",
    "        print(f\"y_test: {y_test[i]}\")\n",
    "        print(f\"y_pred_sentimientos: {y_pred_sentimientos[i]}\")\n",
    "        print()  # Línea en blanco para separar las salidas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modelo exportado correctamente\n"
     ]
    }
   ],
   "source": [
    "# Make something to export the model to another notebook\n",
    "import joblib\n",
    "\n",
    "joblib.dump(modelo_sentimientos, './models/modelo_sentimientos.pkl')\n",
    "\n",
    "print(\"Modelo exportado correctamente\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modelo Recomendaciones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "juegos_dataset = pd.read_csv('./databases/juegos_procesados.csv')\n",
    "libros_dataset = pd.read_csv('./databases/libros_procesados.csv')\n",
    "peliculas_dataset = pd.read_csv('./databases/peliculas_procesados.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelo_recomendaciones = SentenceTransformer('sentence-transformers/all-MiniLM-L12-v2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c308611828b1469ea8d412a45b1fb2cb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 5 files:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Usuario\\OneDrive\\Escritorio\\archivos\\projects\\NLP_Bravi\\.venv\\Lib\\site-packages\\transformers\\convert_slow_tokenizer.py:561: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "modelo_gliner = GLiNER.from_pretrained(\"urchade/gliner_medium-v2.1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "joblib.dump(modelo_recomendaciones, './models/modelo_recomendaciones.pkl')\n",
    "joblib.dump(modelo_gliner, './models/modelo_gliner.pkl')\n",
    "print(\"Modelo exportado correctamente\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def traducir(query):\n",
    "    translated = MyMemoryTranslator(source='spanish', target='english').translate(query)\n",
    "    return translated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "juegos_dataset['embeddings'] = juegos_dataset['description'].apply(lambda x: modelo_recomendaciones.encode(x))\n",
    "libros_dataset['embeddings'] = libros_dataset['description'].apply(lambda x: modelo_recomendaciones.encode(x))\n",
    "peliculas_dataset['embeddings'] = peliculas_dataset['description'].apply(lambda x: modelo_recomendaciones.encode(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "def busqueda_peliculas(consulta):\n",
    "    #convierto los valores de las columnas en listas\n",
    "    peliculas_dataset['actors'] = peliculas_dataset['actors'].apply(lambda x: ast.literal_eval(x) if isinstance(x, str) else x)\n",
    "    peliculas_dataset['categories'] = peliculas_dataset['categories'].apply(lambda x: ast.literal_eval(x) if isinstance(x, str) else x)\n",
    "\n",
    "    consulta_usuario = consulta\n",
    "    labels = ['Name','Genre','Year']\n",
    "    entidades_consulta = modelo_gliner.predict_entities(traducir(consulta_usuario),labels,threshold=0.6)\n",
    "    names = []\n",
    "    years = []\n",
    "    genres = []\n",
    "    for entidad in entidades_consulta:\n",
    "        if entidad['label'] == 'Name':\n",
    "            if entidad['text'].lower() == 'i':\n",
    "                continue\n",
    "            names.append(entidad['text'].lower())\n",
    "        if entidad['label'] == 'Year':\n",
    "            years.append(entidad['text'].lower())\n",
    "        if entidad['label'] == 'Genre':\n",
    "            genres.append(entidad['text'].lower())\n",
    "\n",
    "    candidate_movies = peliculas_dataset.copy()\n",
    "    # Filtrar por nombres en 'director' o 'actors'\n",
    "    if len(names) > 0:\n",
    "        candidate_movies = candidate_movies = peliculas_dataset[\n",
    "        peliculas_dataset.apply(lambda row: any(name in [actor for actor in row['actors']] or name in [director for director in row['director']] for name in names), axis=1)]\n",
    "     # Filtrar por género en 'categories' (solo si hay géneros extraídos)\n",
    "    if len(genres) > 0:\n",
    "        candidate_movies = candidate_movies[\n",
    "            candidate_movies.apply(lambda row: any(genre in [category for category in row['categories']] for genre in genres), axis=1)\n",
    "        ]\n",
    "\n",
    "    # Filtrar por años en 'years' (solo si hay años extraídos)\n",
    "    if len(years) > 0:\n",
    "        candidate_movies = candidate_movies[\n",
    "            candidate_movies.apply(lambda row: any(str(year) in str(row['year']) for year in years), axis=1)\n",
    "        ]\n",
    "\n",
    "    # Mostrar el resultado final\n",
    "    user_embedding = modelo_recomendaciones.encode(traducir(consulta_usuario),convert_to_tensor=True)\n",
    "\n",
    "    coseno = util.cos_sim(user_embedding, candidate_movies['embeddings'].tolist())[0]\n",
    "    index = coseno.argsort(descending=True)[:3].tolist()\n",
    "    # Extraer las recomendaciones\n",
    "    recomendaciones = [candidate_movies['title'].iloc[i] for i in index]\n",
    "    return recomendaciones\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "def busqueda_libros(consulta):\n",
    "    #convierto los valores de las columnas en listas\n",
    "    libros_dataset['categories'] = libros_dataset['categories'].apply(lambda x: ast.literal_eval(x) if isinstance(x, str) else x)\n",
    "\n",
    "    consulta_usuario = consulta\n",
    "    labels = ['Genre']\n",
    "    entidades_consulta = modelo_gliner.predict_entities(traducir(consulta_usuario),labels,threshold=0.6)\n",
    "    genres = []\n",
    "    for entidad in entidades_consulta:\n",
    "\n",
    "        if entidad['label'] == 'Genre':\n",
    "            genres.append(entidad['text'].lower())\n",
    "\n",
    "    candidate_movies = libros_dataset.copy()\n",
    "\n",
    "     # Filtrar por género en 'categories' (solo si hay géneros extraídos)\n",
    "    if len(genres) > 0:\n",
    "        candidate_movies = candidate_movies[\n",
    "            candidate_movies.apply(lambda row: any(genre in [category for category in row['categories']] for genre in genres), axis=1)\n",
    "        ]\n",
    "\n",
    "\n",
    "    # Mostrar el resultado final\n",
    "    user_embedding = modelo_recomendaciones.encode(traducir(consulta_usuario),convert_to_tensor=True)\n",
    "\n",
    "    coseno = util.cos_sim(user_embedding, candidate_movies['embeddings'].tolist())[0]\n",
    "    index = coseno.argsort(descending=True)[:3].tolist()\n",
    "    # Extraer las recomendaciones\n",
    "    recomendaciones = [candidate_movies['title'].iloc[i] for i in index]\n",
    "    return recomendaciones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "def busqueda_juegos(consulta):\n",
    "    #convierto los valores de las columnas en listas\n",
    "    juegos_dataset['directors'] = juegos_dataset['directors'].apply(lambda x: ast.literal_eval(x) if isinstance(x, str) else x)\n",
    "    juegos_dataset['categories'] = juegos_dataset['categories'].apply(lambda x: ast.literal_eval(x) if isinstance(x, str) else x)\n",
    "\n",
    "    consulta_usuario = consulta\n",
    "    labels = ['Name','Genre','Year','Numplayers']\n",
    "    entidades_consulta = modelo_gliner.predict_entities(traducir(consulta_usuario),labels,threshold=0.6)\n",
    "    names = []\n",
    "    years = []\n",
    "    genres = []\n",
    "    numplayers = 1\n",
    "    for entidad in entidades_consulta:\n",
    "        if entidad['label'] == 'Name':\n",
    "            if entidad['text'].lower() == 'i':\n",
    "                continue\n",
    "            names.append(entidad['text'].lower())\n",
    "        if entidad['label'] == 'Year':\n",
    "            years.append(entidad['text'].lower())\n",
    "        if entidad['label'] == 'Genre':\n",
    "            genres.append(entidad['text'].lower())\n",
    "        if entidad['label'] == 'Numplayers':\n",
    "            numplayers = int(entidad['text'][0])\n",
    "\n",
    "    candidate_movies = juegos_dataset.copy()\n",
    "    # Filtrar por nombres en 'director' o 'actors'\n",
    "    if len(names) > 0:\n",
    "        candidate_movies = candidate_movies = juegos_dataset[\n",
    "        juegos_dataset.apply(lambda row: any(name in [director for director in row['directors']] for name in names), axis=1)]\n",
    "     # Filtrar por género en 'categories' (solo si hay géneros extraídos)\n",
    "    if len(genres) > 0:\n",
    "        candidate_movies = candidate_movies[\n",
    "            candidate_movies.apply(lambda row: any(genre in [category for category in row['categories']] for genre in genres), axis=1)\n",
    "        ]\n",
    "\n",
    "    # Filtrar por años en 'years' (solo si hay años extraídos)\n",
    "    if len(years) > 0:\n",
    "        candidate_movies = candidate_movies[\n",
    "            candidate_movies.apply(lambda row: any(str(year) in str(row['year']) for year in years), axis=1)\n",
    "        ]\n",
    "    #Filtrar por la cantidad de jugadores\n",
    "    candidate_movies.apply(lambda row: row['minplayers'] <= numplayers <= row['maxplayers'], axis=1)\n",
    "    # Mostrar el resultado final\n",
    "    user_embedding = modelo_recomendaciones.encode(traducir(consulta_usuario),convert_to_tensor=True)\n",
    "\n",
    "    coseno = util.cos_sim(user_embedding, candidate_movies['embeddings'].tolist())[0]\n",
    "    index = coseno.argsort(descending=True)[:3].tolist()\n",
    "    # Extraer las recomendaciones\n",
    "    recomendaciones = [candidate_movies['title'].iloc[i] for i in index]\n",
    "    return recomendaciones\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Horrified', 'Zombicide Season 2: Prison Outbreak', 'Reign of Cthulhu']\n",
      "['The Mysteries of Udolpho', 'Phantastes: A Faerie Romance for Men and Women', 'The Blue Fairy Book']\n",
      "['Furious Seven', 'The Last Witch Hunter']\n"
     ]
    }
   ],
   "source": [
    "print(busqueda_juegos('quiero un juego que se pueda jugar de a 3 personas y que sea de horror'))\n",
    "print(busqueda_libros('Quiero leer un libro de misterio y fantasia'))\n",
    "print(busqueda_peliculas('Quiero que la historia sea de accion y que actue Vin Diesel y sea del 2015'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
